{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joeljoyjj12/NeRF-Minimal/blob/main/NeRF_Minimal_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFOdezTWzvkX"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfG65dJirxTH"
      },
      "outputs": [],
      "source": [
        "# Setting random seed to obtain reproducible results.\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import imageio\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize global variables.\n",
        "AUTO = tf.data.AUTOTUNE\n",
        "BATCH_SIZE = 5\n",
        "NUM_SAMPLES =  32\n",
        "POS_ENCODE_DIMS = 16\n",
        "EPOCHS = 800\n",
        "\n",
        "name='hotdog' # name of data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YgMbnPpzj_S"
      },
      "source": [
        "###Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlaHalk6r-FA"
      },
      "outputs": [],
      "source": [
        "# ------------------------------ Original Data Berkeley----------------------------------------#\n",
        "# Download the data if it does not already exist.\n",
        "# file_name = \"tiny_nerf_data.npz\"\n",
        "# url = \"https://people.eecs.berkeley.edu/~bmild/nerf/tiny_nerf_data.npz\"\n",
        "# if not os.path.exists(file_name):\n",
        "#     data = keras.utils.get_file(fname=file_name, origin=url)\n",
        "\n",
        "# data = np.load(data) # data from url\n",
        "#--------------------------------------------------------------------------------------------#\n",
        "\n",
        "# Custom Data \n",
        "data = np.load(f'/content/{name}_data.npy',allow_pickle=True)\n",
        "\n",
        "# Loading data\n",
        "data=data.item()\n",
        "print(type(data))\n",
        "\n",
        "\n",
        "print(f'Keys are : {[k for k in data.keys()]}')  # Keys in data\n",
        "\n",
        "images = data[\"images\"]\n",
        "im_shape = images.shape\n",
        "\n",
        "print(f\"Image shape : {im_shape}\")\n",
        "\n",
        "(num_images, H, W, _) = images.shape\n",
        "(poses, focal) = (data[\"poses\"], data[\"focal\"])   # each pose is 4x4 array\n",
        "\n",
        "# Plot a random image from the dataset for visualization.\n",
        "# rand_int=np.random.randint(0,num_images)\n",
        "rand_int=0\n",
        "\n",
        "print(f'Image {rand_int} :')\n",
        "plt.imshow(images[rand_int])\n",
        "plt.show()\n",
        "\n",
        "plt.imsave(f'{name}_sample.jpeg',images[rand_int])\n",
        "\n",
        "print(poses[rand_int])\n",
        "print(focal)\n",
        "\n",
        "poses=poses.astype('float32')\n",
        "print(poses.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Zp26vy340IU"
      },
      "source": [
        "#### Encoding position into Higher dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ip4RthVY0LWQ"
      },
      "outputs": [],
      "source": [
        "def encode_position(x):\n",
        "    \"\"\"Encodes the position into its corresponding Fourier feature.\n",
        "\n",
        "    Args:\n",
        "        x: The input coordinate.\n",
        "\n",
        "    Returns:\n",
        "        Fourier features tensors of the position.\n",
        "    \"\"\"\n",
        "\n",
        "    # print(\"Inside encode : \",x.shape)\n",
        "\n",
        "    positions = [x]\n",
        "    for i in range(POS_ENCODE_DIMS):\n",
        "        for fn in [tf.sin, tf.cos]:\n",
        "            positions.append(fn(2.0 ** i * x))\n",
        "\n",
        "    # print(\"Inside encode : \",len(positions),len(positions[0]),len(positions[0][0]))\n",
        "    return tf.concat(positions, axis=-1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7u7PA0U771_j"
      },
      "source": [
        "#### Origin point and direction vector of Rays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEGYTRzQ6xbg"
      },
      "outputs": [],
      "source": [
        "def get_rays(height, width, focal, pose):\n",
        "    \"\"\"Computes origin point and direction vector of rays.\n",
        "\n",
        "    Args:\n",
        "        height: Height of the image.\n",
        "        width: Width of the image.\n",
        "        focal: The focal length between the images and the camera.\n",
        "        pose: The pose matrix of the camera.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of origin point and direction vector for rays.\n",
        "    \"\"\"\n",
        "    # Build a meshgrid for the rays.\n",
        "    i, j = tf.meshgrid(\n",
        "        tf.range(width, dtype=tf.float32),\n",
        "        tf.range(height, dtype=tf.float32),\n",
        "        indexing=\"xy\",\n",
        "    )\n",
        "\n",
        "\n",
        "    # Normalize the x axis coordinates.\n",
        "    transformed_i = (i - width * 0.5) / focal\n",
        "\n",
        "    # Normalize the y axis coordinates.\n",
        "    transformed_j = (j - height * 0.5) / focal\n",
        "\n",
        "    # Create the direction unit vectors.\n",
        "    directions = tf.stack([transformed_i, -transformed_j, -tf.ones_like(i)], axis=-1)\n",
        "\n",
        "    # Get the camera matrix.\n",
        "    camera_matrix = pose[:3, :3]\n",
        "    height_width_focal = pose[:3, -1]\n",
        "\n",
        "    # Get origins and directions for the rays.\n",
        "    transformed_dirs = directions[..., None, :]\n",
        "    camera_dirs = transformed_dirs * camera_matrix\n",
        "    ray_directions = tf.reduce_sum(camera_dirs, axis=-1)\n",
        "    ray_origins = tf.broadcast_to(height_width_focal, tf.shape(ray_directions))\n",
        "\n",
        "    # Return the origins and directions.\n",
        "    return (ray_origins, ray_directions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a6M7LaIAjFj"
      },
      "source": [
        "#### Rendering and Flattening ray"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCnYg0j47-fq"
      },
      "outputs": [],
      "source": [
        "def render_flat_rays(ray_origins, ray_directions, near, far, num_samples, rand=False):\n",
        "    \"\"\"Renders the rays and flattens it.\n",
        "\n",
        "    Args:\n",
        "        ray_origins: The origin points for rays.\n",
        "        ray_directions: The direction unit vectors for the rays.\n",
        "        near: The near bound of the volumetric scene.\n",
        "        far: The far bound of the volumetric scene.\n",
        "        num_samples: Number of sample points in a ray.\n",
        "        rand: Choice for randomising the sampling strategy.\n",
        "\n",
        "    Returns:\n",
        "       Tuple of flattened rays and sample points on each rays.\n",
        "    \"\"\"\n",
        "    # Compute 3D query points.\n",
        "    # Equation: r(t) = o+td -> Building the \"t\" here.\n",
        "    t_vals = tf.linspace(near, far, num_samples)\n",
        "\n",
        "    if rand:\n",
        "        # Inject uniform noise into sample space to make the sampling\n",
        "        # continuous.\n",
        "        shape = list(ray_origins.shape[:-1]) + [num_samples]\n",
        "        noise = tf.random.uniform(shape=shape) * (far - near) / num_samples\n",
        "\n",
        "        # print(\"t_vals before\")\n",
        "        # print(t_vals.shape)\n",
        "\n",
        "        t_vals = t_vals + noise\n",
        "\n",
        "        # print(\"t_vals after\")\n",
        "        # print(t_vals.shape)\n",
        "\n",
        "    # Equation: r(t) = o + td -> Building the \"r\" here.\n",
        "    rays = ray_origins[..., None, :] + (\n",
        "        ray_directions[..., None, :] * t_vals[..., None]\n",
        "    )\n",
        "    rays_flat = tf.reshape(rays, [-1, 3])\n",
        "\n",
        "\n",
        "    #$$$$$$$$$$$$$$$$$$$$$$$$$$$###################\n",
        "    print(rays.shape)\n",
        "    print(rays_flat.shape)\n",
        "\n",
        "\n",
        "    rays_flat = encode_position(rays_flat)\n",
        "\n",
        "    #$$$$$$$$$$$$$$$$$$$$$$$$$$$$####################\n",
        "    print(\"rays_pos_encode shape : \",rays_flat.shape)\n",
        "\n",
        "    return (rays_flat, t_vals)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHuvH9HPA7zz"
      },
      "source": [
        "#### Mapping individual pose to flattened rays and sampling it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBr-8kf2Auya"
      },
      "outputs": [],
      "source": [
        "# ---------  Original near = 2.0 far=6.0\n",
        "def map_fn(pose,near=1.8,far=6.0):\n",
        "    \"\"\"Maps individual pose to flattened rays and sample points.\n",
        "\n",
        "    Args:\n",
        "        pose: The pose matrix of the camera.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of flattened rays and sample points corresponding to the\n",
        "        camera pose.\n",
        "    \"\"\"\n",
        "    (ray_origins, ray_directions) = get_rays(height=H, width=W, focal=focal, pose=pose)\n",
        "    (rays_flat, t_vals) = render_flat_rays(\n",
        "        ray_origins=ray_origins,\n",
        "        ray_directions=ray_directions,\n",
        "        near=near,# change made\n",
        "        far=far,  # change made\n",
        "        num_samples=NUM_SAMPLES,\n",
        "        rand=True,\n",
        "    )\n",
        "    return (rays_flat, t_vals)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00i3dC09BEk5"
      },
      "source": [
        "### Preparing images for training and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0P5-9gGRBA3A"
      },
      "outputs": [],
      "source": [
        "# Create the training split.\n",
        "split_index = int(num_images * 0.9)\n",
        "\n",
        "# Split the images into training and validation.\n",
        "train_images = images[:split_index]\n",
        "val_images = images[split_index:]\n",
        "\n",
        "# Split the poses into training and validation.\n",
        "train_poses = poses[:split_index]\n",
        "val_poses = poses[split_index:]\n",
        "\n",
        "# Make the training pipeline.\n",
        "train_img_ds = tf.data.Dataset.from_tensor_slices(train_images)\n",
        "train_pose_ds = tf.data.Dataset.from_tensor_slices(train_poses)\n",
        "train_ray_ds = train_pose_ds.map(map_fn, num_parallel_calls=AUTO)\n",
        "training_ds = tf.data.Dataset.zip((train_img_ds, train_ray_ds))\n",
        "train_ds = (\n",
        "    training_ds.shuffle(BATCH_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True, num_parallel_calls=AUTO)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "# Make the validation pipeline.\n",
        "val_img_ds = tf.data.Dataset.from_tensor_slices(val_images)\n",
        "val_pose_ds = tf.data.Dataset.from_tensor_slices(val_poses)\n",
        "val_ray_ds = val_pose_ds.map(map_fn, num_parallel_calls=AUTO)\n",
        "validation_ds = tf.data.Dataset.zip((val_img_ds, val_ray_ds))\n",
        "val_ds = (\n",
        "    validation_ds.shuffle(BATCH_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True, num_parallel_calls=AUTO)\n",
        "    .prefetch(AUTO)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbfL6sx6Si8j"
      },
      "outputs": [],
      "source": [
        "# print(train_images.shape)\n",
        "# print(train_poses.shape)\n",
        "# print(len(train_pose_ds),train_pose_ds)\n",
        "# print(len(train_img_ds),train_img_ds)\n",
        "\n",
        "# print(\"\\n \\n Train ray_ds \\n\")\n",
        "# print(len(train_ray_ds),train_ray_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUPzbQRwGJQJ"
      },
      "source": [
        "# Creating the NeRF Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhfMeGNDBJfO"
      },
      "outputs": [],
      "source": [
        "def get_nerf_model(num_layers, num_pos):\n",
        "    \"\"\"Generates the NeRF neural network.\n",
        "\n",
        "    Args:\n",
        "        num_layers: The number of MLP layers.\n",
        "        num_pos: The number of dimensions of positional encoding.\n",
        "\n",
        "    Returns:\n",
        "        The [`tf.keras`](https://www.tensorflow.org/api_docs/python/tf/keras) model.\n",
        "    \"\"\"\n",
        "    inputs = keras.Input(shape=(num_pos, 2 * 3 * POS_ENCODE_DIMS + 3))\n",
        "    x = inputs\n",
        "    for i in range(num_layers):\n",
        "        x = layers.Dense(units=128, activation=\"relu\")(x)\n",
        "        if i % 4 == 0 and i > 0:\n",
        "            # Inject residual connection.\n",
        "            x = layers.concatenate([x, inputs], axis=-1)\n",
        "    outputs = layers.Dense(units=4)(x)\n",
        "    return keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "\n",
        "def render_rgb_depth(model, rays_flat, t_vals, rand=True, train=True):\n",
        "    \"\"\"Generates the RGB image and depth map from model prediction.\n",
        "\n",
        "    Args:\n",
        "        model: The MLP model that is trained to predict the rgb and\n",
        "            volume density of the volumetric scene.\n",
        "        rays_flat: The flattened rays that serve as the input to\n",
        "            the NeRF model.\n",
        "        t_vals: The sample points for the rays.\n",
        "        rand: Choice to randomise the sampling strategy.\n",
        "        train: Whether the model is in the training or testing phase.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of rgb image and depth map.\n",
        "    \"\"\"\n",
        "    # Get the predictions from the nerf model and reshape it.\n",
        "    if train:\n",
        "        predictions = model(rays_flat)\n",
        "    else:\n",
        "        predictions = model.predict(rays_flat)\n",
        "    predictions = tf.reshape(predictions, shape=(BATCH_SIZE, H, W, NUM_SAMPLES, 4))\n",
        "\n",
        "    # Slice the predictions into rgb and sigma.\n",
        "    rgb = tf.sigmoid(predictions[..., :-1])\n",
        "    sigma_a = tf.nn.relu(predictions[..., -1])\n",
        "\n",
        "    # Get the distance of adjacent intervals.\n",
        "    delta = t_vals[..., 1:] - t_vals[..., :-1]\n",
        "    # delta shape = (num_samples)\n",
        "    if rand:\n",
        "        delta = tf.concat(\n",
        "            [delta, tf.broadcast_to([1e10], shape=(BATCH_SIZE, H, W, 1))], axis=-1\n",
        "        )\n",
        "        alpha = 1.0 - tf.exp(-sigma_a * delta)\n",
        "    else:\n",
        "        delta = tf.concat(\n",
        "            [delta, tf.broadcast_to([1e10], shape=(BATCH_SIZE, 1))], axis=-1\n",
        "        )\n",
        "        alpha = 1.0 - tf.exp(-sigma_a * delta[:, None, None, :])\n",
        "\n",
        "    # Get transmittance.\n",
        "    exp_term = 1.0 - alpha\n",
        "    epsilon = 1e-10\n",
        "    transmittance = tf.math.cumprod(exp_term + epsilon, axis=-1, exclusive=True)\n",
        "    weights = alpha * transmittance\n",
        "    rgb = tf.reduce_sum(weights[..., None] * rgb, axis=-2)\n",
        "\n",
        "    if rand:\n",
        "        depth_map = tf.reduce_sum(weights * t_vals, axis=-1)\n",
        "    else:\n",
        "        depth_map = tf.reduce_sum(weights * t_vals[:, None, None], axis=-1)\n",
        "    return (rgb, depth_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxT5mOSjG5d5"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txwtdym7GO-2"
      },
      "outputs": [],
      "source": [
        "class NeRF(keras.Model):\n",
        "    def __init__(self, nerf_model):\n",
        "        super().__init__()\n",
        "        self.nerf_model = nerf_model\n",
        "\n",
        "    def compile(self, optimizer, loss_fn):\n",
        "        super().compile()\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "        self.psnr_metric = keras.metrics.Mean(name=\"psnr\")\n",
        "\n",
        "    def train_step(self, inputs):\n",
        "        # Get the images and the rays.\n",
        "        (images, rays) = inputs\n",
        "        (rays_flat, t_vals) = rays\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Get the predictions from the model.\n",
        "            rgb, _ = render_rgb_depth(\n",
        "                model=self.nerf_model, rays_flat=rays_flat, t_vals=t_vals, rand=True\n",
        "            )\n",
        "            loss = self.loss_fn(images, rgb)\n",
        "\n",
        "        # Get the trainable variables.\n",
        "        trainable_variables = self.nerf_model.trainable_variables\n",
        "\n",
        "        # Get the gradeints of the trainiable variables with respect to the loss.\n",
        "        gradients = tape.gradient(loss, trainable_variables)\n",
        "\n",
        "        # Apply the grads and optimize the model.\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "\n",
        "        # Get the PSNR of the reconstructed images and the source images.\n",
        "        psnr = tf.image.psnr(images, rgb, max_val=1.0)\n",
        "\n",
        "        # Compute our own metrics\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        self.psnr_metric.update_state(psnr)\n",
        "        return {\"loss\": self.loss_tracker.result(), \"psnr\": self.psnr_metric.result()}\n",
        "\n",
        "    def test_step(self, inputs):\n",
        "        # Get the images and the rays.\n",
        "        (images, rays) = inputs\n",
        "        (rays_flat, t_vals) = rays\n",
        "\n",
        "        # Get the predictions from the model.\n",
        "        rgb, _ = render_rgb_depth(\n",
        "            model=self.nerf_model, rays_flat=rays_flat, t_vals=t_vals, rand=True\n",
        "        )\n",
        "        loss = self.loss_fn(images, rgb)\n",
        "\n",
        "        # Get the PSNR of the reconstructed images and the source images.\n",
        "        psnr = tf.image.psnr(images, rgb, max_val=1.0)\n",
        "\n",
        "        # Compute our own metrics\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        self.psnr_metric.update_state(psnr)\n",
        "        return {\"loss\": self.loss_tracker.result(), \"psnr\": self.psnr_metric.result()}\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_tracker, self.psnr_metric]\n",
        "\n",
        "\n",
        "test_imgs, test_rays = next(iter(train_ds))\n",
        "test_rays_flat, test_t_vals = test_rays\n",
        "\n",
        "loss_list = []\n",
        "\n",
        "\n",
        "class TrainMonitor(keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        loss = logs[\"loss\"]\n",
        "        loss_list.append(loss)\n",
        "        test_recons_images, depth_maps = render_rgb_depth(\n",
        "            model=self.model.nerf_model,\n",
        "            rays_flat=test_rays_flat,\n",
        "            t_vals=test_t_vals,\n",
        "            rand=True,\n",
        "            train=False,\n",
        "        )\n",
        "\n",
        "        # Plot the rgb, depth and the loss plot.\n",
        "\n",
        "        #------------------------ Plotting part -----------------------------------#\n",
        "        if epoch%3==0:\n",
        "          fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 5))\n",
        "          ax[0].imshow(keras.preprocessing.image.array_to_img(test_recons_images[0]))\n",
        "          ax[0].set_title(f\"Predicted Image: {epoch:04d}\")\n",
        "\n",
        "          ax[1].imshow(keras.preprocessing.image.array_to_img(depth_maps[0, ..., None]))\n",
        "          ax[1].set_title(f\"Depth Map: {epoch:04d}\")\n",
        "\n",
        "          ax[2].plot(loss_list)\n",
        "          ax[2].set_xticks(np.arange(0, EPOCHS + 1, 5.0))\n",
        "          ax[2].set_title(f\"Loss Plot: {epoch:04d}\")\n",
        "\n",
        "          fig.savefig(f\"{name}_images/{epoch:03d}.png\")\n",
        "          plt.show()\n",
        "          plt.close()\n",
        "        #--------------------------------------------------------------------------#\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEKuFuAnsrCp"
      },
      "outputs": [],
      "source": [
        "print(H,W)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHzNJE9Df6uM"
      },
      "outputs": [],
      "source": [
        "#------------------------------------------------------------------------------#\n",
        "#To save weights during each iteration\n",
        "\n",
        "checkpoint_filepath = f'weights_500_{name}.ckpt'\n",
        "checkp = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    verbose=1,\n",
        "    save_weights_only=True)\n",
        "\n",
        "#------------------------------------------------------------------------------#\n",
        "\n",
        "num_pos = H * W * NUM_SAMPLES\n",
        "\n",
        "nerf_model = get_nerf_model(num_layers=8, num_pos=num_pos) # Creating new model\n",
        "\n",
        "# ----------------------Loading saved model------------------------#\n",
        "# nerf_model = keras.models.load_model(f'/content/{name}_NeRF_766.h5')\n",
        "#------------------------------------------------------------------# \n",
        "\n",
        "#---------------------------------#\n",
        "# nerf_model.load_weights('/content/weights_1000_silica.h5')\n",
        "#---------------------------------#\n",
        "\n",
        "\n",
        "model = NeRF(nerf_model)\n",
        "\n",
        "#---------------***Loading Weights***------------------------------------------#\n",
        "model.load_weights(f'weights_766_{name}.ckpt')\n",
        "#------------------------------------------------------------------------------#\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(), loss_fn=keras.losses.MeanSquaredError()\n",
        ")\n",
        "\n",
        "# Create a directory to save the images during training.\n",
        "if not os.path.exists(f\"{name}_images\"):\n",
        "    os.makedirs(f\"{name}_images\")\n",
        "\n",
        "model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[TrainMonitor(),checkp],\n",
        "    steps_per_epoch=split_index // BATCH_SIZE,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0O3AmkLSswzk"
      },
      "source": [
        "#### Model Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csiaf3ObcUyo"
      },
      "outputs": [],
      "source": [
        "model = keras.models.load_model('/content/NeRF_2000_iter_silica.h5')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7wbnIXTd6Kz"
      },
      "source": [
        "#### GIF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXM3Mzjjd5hu"
      },
      "outputs": [],
      "source": [
        "def create_gif(path_to_images, name_gif):\n",
        "    filenames = glob.glob(path_to_images)\n",
        "    filenames = sorted(filenames)\n",
        "    images = []\n",
        "    for filename in tqdm(filenames):\n",
        "        images.append(imageio.imread(filename))\n",
        "    kargs = {\"duration\": 0.2}\n",
        "    imageio.mimsave(name_gif, images, \"GIF\", **kargs)\n",
        "\n",
        "create_gif(f\"{name}_images/*.png\", f\"training_{name}.gif\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VSnhmPMeyEA"
      },
      "source": [
        "#### Saving Model and Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fXrOoGPe0hH"
      },
      "outputs": [],
      "source": [
        "tf.keras.models.save_model(nerf_model,f'{name}_NeRF_1000.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3EDWKxWHsdf"
      },
      "source": [
        "# Outputs and Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wExheKzrHmWo"
      },
      "outputs": [],
      "source": [
        "# Get the trained NeRF model and infer.\n",
        "nerf_model = model.nerf_model\n",
        "test_recons_images, depth_maps = render_rgb_depth(\n",
        "    model=nerf_model,\n",
        "    rays_flat=test_rays_flat,\n",
        "    t_vals=test_t_vals,\n",
        "    rand=True,\n",
        "    train=False,\n",
        ")\n",
        "\n",
        "# Create subplots.\n",
        "fig, axes = plt.subplots(nrows=5, ncols=3, figsize=(10, 20))\n",
        "\n",
        "for ax, ori_img, recons_img, depth_map in zip(\n",
        "    axes, test_imgs, test_recons_images, depth_maps\n",
        "):\n",
        "    ax[0].imshow(keras.preprocessing.image.array_to_img(ori_img))\n",
        "    ax[0].set_title(\"Original\")\n",
        "\n",
        "    ax[1].imshow(keras.preprocessing.image.array_to_img(recons_img))\n",
        "    ax[1].set_title(\"Reconstructed\")\n",
        "\n",
        "    ax[2].imshow(\n",
        "        keras.preprocessing.image.array_to_img(depth_map[..., None]), cmap=\"inferno\"\n",
        "    )\n",
        "    ax[2].set_title(\"Depth Map\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "havjk7eGgZfM"
      },
      "outputs": [],
      "source": [
        "# Saving original and reconstructed images\n",
        "tf.keras.utils.save_img(f'{name}_reconstructed.png',test_recons_images[0])\n",
        "tf.keras.utils.save_img(f'{name}_original.png',test_imgs[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRrAGVqJIu6h"
      },
      "source": [
        "## 360 degree Video Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rqEQWwUHwaq"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_translation_t(t):\n",
        "    \"\"\"Get the translation matrix for movement in t.\"\"\"\n",
        "    matrix = [\n",
        "        [1, 0, 0, 0],\n",
        "        [0, 1, 0, 0],\n",
        "        [0, 0, 1, t],\n",
        "        [0, 0, 0, 1],\n",
        "    ]\n",
        "    return tf.convert_to_tensor(matrix, dtype=tf.float32)\n",
        "\n",
        "\n",
        "def get_rotation_phi(phi):\n",
        "    \"\"\"Get the rotation matrix for movement in phi.\"\"\"\n",
        "    matrix = [\n",
        "        [1, 0, 0, 0],\n",
        "        [0, tf.cos(phi), -tf.sin(phi), 0],\n",
        "        [0, tf.sin(phi), tf.cos(phi), 0],\n",
        "        [0, 0, 0, 1],\n",
        "    ]\n",
        "    return tf.convert_to_tensor(matrix, dtype=tf.float32)\n",
        "\n",
        "\n",
        "def get_rotation_theta(theta):\n",
        "    \"\"\"Get the rotation matrix for movement in theta.\"\"\"\n",
        "    matrix = [\n",
        "        [tf.cos(theta), 0, -tf.sin(theta), 0],\n",
        "        [0, 1, 0, 0],\n",
        "        [tf.sin(theta), 0, tf.cos(theta), 0],\n",
        "        [0, 0, 0, 1],\n",
        "    ]\n",
        "    return tf.convert_to_tensor(matrix, dtype=tf.float32)\n",
        "\n",
        "\n",
        "def pose_spherical(theta, phi, t):\n",
        "    \"\"\"\n",
        "    Get the camera to world matrix for the corresponding theta, phi\n",
        "    and t.\n",
        "    \"\"\"\n",
        "    c2w = get_translation_t(t)\n",
        "    c2w = get_rotation_phi(phi / 180.0 * np.pi) @ c2w\n",
        "    c2w = get_rotation_theta(theta / 180.0 * np.pi) @ c2w\n",
        "    c2w = np.array([[-1, 0, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0, 0, 0, 1]]) @ c2w\n",
        "    return c2w\n",
        "\n",
        "\n",
        "rgb_frames = []\n",
        "batch_flat = []\n",
        "batch_t = []\n",
        "\n",
        "# Iterate over different theta value and generate scenes.\n",
        "for index, c2w in tqdm(enumerate(poses)):\n",
        "# for index, theta in tqdm(enumerate(np.linspace(0.0, 360.0, 360, endpoint=False))):\n",
        "    # Get the camera to world matrix.\n",
        "    # c2w = pose_spherical(theta, -20.0, 4.0)\n",
        "\n",
        "    #\n",
        "    ray_oris, ray_dirs = get_rays(H, W, focal, c2w)\n",
        "    rays_flat, t_vals = render_flat_rays(\n",
        "        ray_oris, ray_dirs, near=1.8, far=6.0, num_samples=NUM_SAMPLES, rand=False\n",
        "    )\n",
        "\n",
        "    if index % BATCH_SIZE == 0 and index > 0:\n",
        "        batched_flat = tf.stack(batch_flat, axis=0)\n",
        "        batch_flat = [rays_flat]\n",
        "\n",
        "        batched_t = tf.stack(batch_t, axis=0)\n",
        "        batch_t = [t_vals]\n",
        "\n",
        "        rgb, _ = render_rgb_depth(\n",
        "            nerf_model, batched_flat, batched_t, rand=False, train=False\n",
        "        )\n",
        "\n",
        "        temp_rgb = [np.clip(255 * img, 0.0, 255.0).astype(np.uint8) for img in rgb]\n",
        "\n",
        "        rgb_frames = rgb_frames + temp_rgb\n",
        "    else:\n",
        "        batch_flat.append(rays_flat)\n",
        "        batch_t.append(t_vals)\n",
        "\n",
        "rgb_video = f\"rgb_video_{name}.mp4\"\n",
        "imageio.mimwrite(rgb_video, rgb_frames, fps=30, quality=9, macro_block_size=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htDyKtD_lUNP"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "NeRF_Minimal_2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}